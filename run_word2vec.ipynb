{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Word2Vec Model\n",
    "==============\n",
    "\n",
    "Introduces Gensim's Word2Vec model and demonstrates its use on the `Lee Evaluation Corpus\n",
    "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you missed the buzz, Word2Vec is a widely used algorithm based on neural\n",
    "networks, commonly referred to as \"deep learning\" (though word2vec itself is rather shallow).\n",
    "Using large amounts of unannotated plain text, word2vec learns relationships\n",
    "between words automatically. The output are vectors, one vector per word,\n",
    "with remarkable linear relationships that allow us to do things like:\n",
    "\n",
    "* vec(\"king\") - vec(\"man\") + vec(\"woman\") =~ vec(\"queen\")\n",
    "* vec(\"Montreal Canadiens\") – vec(\"Montreal\") + vec(\"Toronto\") =~ vec(\"Toronto Maple Leafs\").\n",
    "\n",
    "Word2vec is very useful in `automatic text tagging\n",
    "<https://github.com/RaRe-Technologies/movie-plots-by-genre>`_\\ , recommender\n",
    "systems and machine translation.\n",
    "\n",
    "This tutorial:\n",
    "\n",
    "#. Introduces ``Word2Vec`` as an improvement over traditional bag-of-words\n",
    "#. Shows off a demo of ``Word2Vec`` using a pre-trained model\n",
    "#. Demonstrates training a new model from your own data\n",
    "#. Demonstrates loading and saving models\n",
    "#. Introduces several training parameters and demonstrates their effect\n",
    "#. Discusses memory requirements\n",
    "#. Visualizes Word2Vec embeddings by applying dimensionality reduction\n",
    "\n",
    "Review: Bag-of-words\n",
    "--------------------\n",
    "\n",
    ".. Note:: Feel free to skip these review sections if you're already familiar with the models.\n",
    "\n",
    "You may be familiar with the `bag-of-words model\n",
    "<https://en.wikipedia.org/wiki/Bag-of-words_model>`_ from the\n",
    "`core_concepts_vector` section.\n",
    "This model transforms each document to a fixed-length vector of integers.\n",
    "For example, given the sentences:\n",
    "\n",
    "- ``John likes to watch movies. Mary likes movies too.``\n",
    "- ``John also likes to watch football games. Mary hates football.``\n",
    "\n",
    "The model outputs the vectors:\n",
    "\n",
    "- ``[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]``\n",
    "- ``[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]``\n",
    "\n",
    "Each vector has 10 elements, where each element counts the number of times a\n",
    "particular word occurred in the document.\n",
    "The order of elements is arbitrary.\n",
    "In the example above, the order of the elements corresponds to the words:\n",
    "``[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\", \"hates\"]``.\n",
    "\n",
    "Bag-of-words models are surprisingly effective, but have several weaknesses.\n",
    "\n",
    "First, they lose all information about word order: \"John likes Mary\" and\n",
    "\"Mary likes John\" correspond to identical vectors. There is a solution: bag\n",
    "of `n-grams <https://en.wikipedia.org/wiki/N-gram>`__\n",
    "models consider word phrases of length n to represent documents as\n",
    "fixed-length vectors to capture local word order but suffer from data\n",
    "sparsity and high dimensionality.\n",
    "\n",
    "Second, the model does not attempt to learn the meaning of the underlying\n",
    "words, and as a consequence, the distance between vectors doesn't always\n",
    "reflect the difference in meaning.  The ``Word2Vec`` model addresses this\n",
    "second problem.\n",
    "\n",
    "Introducing: the ``Word2Vec`` Model\n",
    "-----------------------------------\n",
    "\n",
    "``Word2Vec`` is a more recent model that embeds words in a lower-dimensional\n",
    "vector space using a shallow neural network. The result is a set of\n",
    "word-vectors where vectors close together in vector space have similar\n",
    "meanings based on context, and word-vectors distant to each other have\n",
    "differing meanings. For example, ``strong`` and ``powerful`` would be close\n",
    "together and ``strong`` and ``Paris`` would be relatively far.\n",
    "\n",
    "The are two versions of this model and :py:class:`~gensim.models.word2vec.Word2Vec`\n",
    "class implements them both:\n",
    "\n",
    "1. Skip-grams (SG)\n",
    "2. Continuous-bag-of-words (CBOW)\n",
    "\n",
    ".. Important::\n",
    "  Don't let the implementation details below scare you.\n",
    "  They're advanced material: if it's too much, then move on to the next section.\n",
    "\n",
    "The `Word2Vec Skip-gram <http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model>`__\n",
    "model, for example, takes in pairs (word1, word2) generated by moving a\n",
    "window across text data, and trains a 1-hidden-layer neural network based on\n",
    "the synthetic task of given an input word, giving us a predicted probability\n",
    "distribution of nearby words to the input. A virtual `one-hot\n",
    "<https://en.wikipedia.org/wiki/One-hot>`__ encoding of words\n",
    "goes through a 'projection layer' to the hidden layer; these projection\n",
    "weights are later interpreted as the word embeddings. So if the hidden layer\n",
    "has 300 neurons, this network will give us 300-dimensional word embeddings.\n",
    "\n",
    "Continuous-bag-of-words Word2vec is very similar to the skip-gram model. It\n",
    "is also a 1-hidden-layer neural network. The synthetic training task now uses\n",
    "the average of multiple input context words, rather than a single word as in\n",
    "skip-gram, to predict the center word. Again, the projection weights that\n",
    "turn one-hot words into averageable vectors, of the same width as the hidden\n",
    "layer, are interpreted as the word embeddings.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec Demo\n",
    "-------------\n",
    "\n",
    "To see what ``Word2Vec`` can do, let's download a pre-trained model and play\n",
    "around with it. We will fetch the Word2Vec model trained on part of the\n",
    "Google News dataset, covering approximately 3 million words and phrases. Such\n",
    "a model can take hours to train, but since it's already available,\n",
    "downloading and loading it with Gensim takes minutes.\n",
    "\n",
    ".. Important::\n",
    "  The model is approximately 2GB, so you'll need a decent network connection\n",
    "  to proceed.  Otherwise, skip ahead to the \"Training Your Own Model\" section\n",
    "  below.\n",
    "\n",
    "You may also check out an `online word2vec demo\n",
    "<http://radimrehurek.com/2014/02/word2vec-tutorial/#app>`_ where you can try\n",
    "this vector algebra for yourself. That demo runs ``word2vec`` on the\n",
    "**entire** Google News dataset, of **about 100 billion words**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 02:27:52,043 : INFO : loading projection weights from /home/shinjini/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
      "2023-05-16 02:29:17,325 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from /home/shinjini/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2023-05-16T02:29:17.322241', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common operation is to retrieve the vocabulary of a model. That is trivial:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #0/3000000 is </s>\n",
      "word #1/3000000 is in\n",
      "word #2/3000000 is for\n",
      "word #3/3000000 is that\n",
      "word #4/3000000 is is\n",
      "word #5/3000000 is on\n",
      "word #6/3000000 is ##\n",
      "word #7/3000000 is The\n",
      "word #8/3000000 is with\n",
      "word #9/3000000 is said\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily obtain vectors for terms the model is familiar with:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king = wv['king']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the model is unable to infer vectors for unfamiliar words.\n",
    "This is one limitation of Word2Vec: if this limitation matters to you, check\n",
    "out the FastText model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'cameroon' does not appear in this model\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vec_cameroon = wv['cameroon']\n",
    "except KeyError:\n",
    "    print(\"The word 'cameroon' does not appear in this model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on, ``Word2Vec`` supports several word similarity tasks out of the\n",
    "box.  You can see how the similarity intuitively decreases as the words get\n",
    "less and less similar.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'minivan'\t0.69\n",
      "'car'\t'bicycle'\t0.54\n",
      "'car'\t'airplane'\t0.42\n",
      "'car'\t'cereal'\t0.14\n",
      "'car'\t'communism'\t0.06\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'cereal'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the 5 most similar words to \"car\" or \"minivan\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SUV', 0.8532192707061768), ('vehicle', 0.8175783753395081), ('pickup_truck', 0.7763688564300537), ('Jeep', 0.7567334175109863), ('Ford_Explorer', 0.7565720081329346)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.most_similar(positive=['car', 'minivan'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the below does not belong in the sequence?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n"
     ]
    }
   ],
   "source": [
    "print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Your Own Model\n",
    "-----------------------\n",
    "\n",
    "To start, you'll need some data for training the model. For the following\n",
    "examples, we'll use the `Lee Evaluation Corpus\n",
    "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
    "(which you `already have\n",
    "<https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/test/test_data/lee_background.cor>`_\n",
    "if you've installed Gensim).\n",
    "\n",
    "This corpus is small enough to fit entirely in memory, but we'll implement a\n",
    "memory-friendly iterator that reads it line-by-line to demonstrate how you\n",
    "would handle a larger corpus.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 02:31:37,148 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2023-05-16 02:31:37,149 : INFO : built Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...> from 9 documents (total 29 corpus positions)\n",
      "2023-05-16 02:31:37,150 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...> from 9 documents (total 29 corpus positions)\", 'datetime': '2023-05-16T02:31:37.150575', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to do any custom preprocessing, e.g. decode a non-standard\n",
    "encoding, lowercase, remove numbers, extract named entities... All of this can\n",
    "be done inside the ``MyCorpus`` iterator and ``word2vec`` doesn’t need to\n",
    "know. All that is required is that the input yields one sentence (list of\n",
    "utf8 words) after another.\n",
    "\n",
    "Let's go ahead and train a model on our corpus.  Don't worry about the\n",
    "training parameters much for now, we'll revisit them later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 02:31:43,375 : INFO : collecting all words and their counts\n",
      "2023-05-16 02:31:43,377 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-05-16 02:31:43,470 : INFO : collected 6981 word types from a corpus of 58152 raw words and 300 sentences\n",
      "2023-05-16 02:31:43,471 : INFO : Creating a fresh vocabulary\n",
      "2023-05-16 02:31:43,485 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1750 unique words (25.07% of original 6981, drops 5231)', 'datetime': '2023-05-16T02:31:43.485761', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2023-05-16 02:31:43,487 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 49335 word corpus (84.84% of original 58152, drops 8817)', 'datetime': '2023-05-16T02:31:43.487174', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2023-05-16 02:31:43,508 : INFO : deleting the raw counts dictionary of 6981 items\n",
      "2023-05-16 02:31:43,510 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2023-05-16 02:31:43,511 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 35935.33721568072 word corpus (72.8%% of prior 49335)', 'datetime': '2023-05-16T02:31:43.511861', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2023-05-16 02:31:43,544 : INFO : estimated required memory for 1750 words and 100 dimensions: 2275000 bytes\n",
      "2023-05-16 02:31:43,545 : INFO : resetting layer weights\n",
      "2023-05-16 02:31:43,549 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-05-16T02:31:43.549375', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'build_vocab'}\n",
      "2023-05-16 02:31:43,550 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1750 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-05-16T02:31:43.550561', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'train'}\n",
      "2023-05-16 02:31:43,703 : INFO : EPOCH 0: training on 58152 raw words (35883 effective words) took 0.1s, 243133 effective words/s\n",
      "2023-05-16 02:31:43,804 : INFO : EPOCH 1: training on 58152 raw words (35889 effective words) took 0.1s, 361497 effective words/s\n",
      "2023-05-16 02:31:43,883 : INFO : EPOCH 2: training on 58152 raw words (35989 effective words) took 0.1s, 466125 effective words/s\n",
      "2023-05-16 02:31:43,961 : INFO : EPOCH 3: training on 58152 raw words (35904 effective words) took 0.1s, 469518 effective words/s\n",
      "2023-05-16 02:31:44,035 : INFO : EPOCH 4: training on 58152 raw words (35933 effective words) took 0.1s, 494692 effective words/s\n",
      "2023-05-16 02:31:44,036 : INFO : Word2Vec lifecycle event {'msg': 'training on 290760 raw words (179598 effective words) took 0.5s, 371110 effective words/s', 'datetime': '2023-05-16T02:31:44.036187', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'train'}\n",
      "2023-05-16 02:31:44,036 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1750, vector_size=100, alpha=0.025>', 'datetime': '2023-05-16T02:31:44.036552', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import gensim.models\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our model, we can use it in the same way as in the demo above.\n",
    "\n",
    "The main part of the model is ``model.wv``\\ , where \"wv\" stands for \"word vectors\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king = model.wv['king']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the vocabulary works the same way:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #0/3000000 is </s>\n",
      "word #1/3000000 is in\n",
      "word #2/3000000 is for\n",
      "word #3/3000000 is that\n",
      "word #4/3000000 is is\n",
      "word #5/3000000 is on\n",
      "word #6/3000000 is ##\n",
      "word #7/3000000 is The\n",
      "word #8/3000000 is with\n",
      "word #9/3000000 is said\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing and loading models\n",
    "--------------------------\n",
    "\n",
    "You'll notice that training non-trivial models can take time.  Once you've\n",
    "trained your model and it works as expected, you can save it to disk.  That\n",
    "way, you don't have to spend time training it all over again later.\n",
    "\n",
    "You can store/load models using the standard gensim methods:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 02:32:02,590 : INFO : Word2Vec lifecycle event {'fname_or_handle': '/tmp/gensim-model-1bvd_i9z', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-05-16T02:32:02.590455', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'saving'}\n",
      "2023-05-16 02:32:02,591 : INFO : not storing attribute cum_table\n",
      "2023-05-16 02:32:02,597 : INFO : saved /tmp/gensim-model-1bvd_i9z\n",
      "2023-05-16 02:32:02,599 : INFO : loading Word2Vec object from /tmp/gensim-model-1bvd_i9z\n",
      "2023-05-16 02:32:02,603 : INFO : loading wv recursively from /tmp/gensim-model-1bvd_i9z.wv.* with mmap=None\n",
      "2023-05-16 02:32:02,604 : INFO : setting ignored attribute cum_table to None\n",
      "2023-05-16 02:32:02,650 : INFO : Word2Vec lifecycle event {'fname': '/tmp/gensim-model-1bvd_i9z', 'datetime': '2023-05-16T02:32:02.649944', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n",
    "    temporary_filepath = tmp.name\n",
    "    model.save(temporary_filepath)\n",
    "    #\n",
    "    # The model is now safely stored in the filepath.\n",
    "    # You can copy it to other machines, share it with others, etc.\n",
    "    #\n",
    "    # To load a saved model:\n",
    "    #\n",
    "    new_model = gensim.models.Word2Vec.load(temporary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which uses pickle internally, optionally ``mmap``\\ ‘ing the model’s internal\n",
    "large NumPy matrices into virtual memory directly from disk files, for\n",
    "inter-process memory sharing.\n",
    "\n",
    "In addition, you can load models created by the original C tool, both using\n",
    "its text and binary formats::\n",
    "\n",
    "  model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)\n",
    "  # using gzipped/bz2 input works too, no need to unzip\n",
    "  model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.bin.gz', binary=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Parameters\n",
    "-------------------\n",
    "\n",
    "``Word2Vec`` accepts several parameters that affect both training speed and quality.\n",
    "\n",
    "min_count\n",
    "---------\n",
    "\n",
    "``min_count`` is for pruning the internal dictionary. Words that appear only\n",
    "once or twice in a billion-word corpus are probably uninteresting typos and\n",
    "garbage. In addition, there’s not enough data to make any meaningful training\n",
    "on those words, so it’s best to ignore them:\n",
    "\n",
    "default value of min_count=5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 02:32:06,408 : INFO : collecting all words and their counts\n",
      "2023-05-16 02:32:06,410 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-05-16 02:32:06,508 : INFO : collected 6981 word types from a corpus of 58152 raw words and 300 sentences\n",
      "2023-05-16 02:32:06,509 : INFO : Creating a fresh vocabulary\n",
      "2023-05-16 02:32:06,516 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 889 unique words (12.73% of original 6981, drops 6092)', 'datetime': '2023-05-16T02:32:06.516483', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2023-05-16 02:32:06,517 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 43776 word corpus (75.28% of original 58152, drops 14376)', 'datetime': '2023-05-16T02:32:06.517361', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2023-05-16 02:32:06,526 : INFO : deleting the raw counts dictionary of 6981 items\n",
      "2023-05-16 02:32:06,526 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2023-05-16 02:32:06,527 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 29691.39528319831 word corpus (67.8%% of prior 43776)', 'datetime': '2023-05-16T02:32:06.527507', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2023-05-16 02:32:06,547 : INFO : estimated required memory for 889 words and 100 dimensions: 1155700 bytes\n",
      "2023-05-16 02:32:06,548 : INFO : resetting layer weights\n",
      "2023-05-16 02:32:06,550 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-05-16T02:32:06.550159', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'build_vocab'}\n",
      "2023-05-16 02:32:06,551 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 889 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-05-16T02:32:06.551060', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'train'}\n",
      "2023-05-16 02:32:06,661 : INFO : EPOCH 0: training on 58152 raw words (29735 effective words) took 0.1s, 276844 effective words/s\n",
      "2023-05-16 02:32:06,802 : INFO : EPOCH 1: training on 58152 raw words (29759 effective words) took 0.1s, 212753 effective words/s\n",
      "2023-05-16 02:32:06,876 : INFO : EPOCH 2: training on 58152 raw words (29595 effective words) took 0.1s, 409877 effective words/s\n",
      "2023-05-16 02:32:06,943 : INFO : EPOCH 3: training on 58152 raw words (29725 effective words) took 0.1s, 455093 effective words/s\n",
      "2023-05-16 02:32:07,012 : INFO : EPOCH 4: training on 58152 raw words (29766 effective words) took 0.1s, 438552 effective words/s\n",
      "2023-05-16 02:32:07,012 : INFO : Word2Vec lifecycle event {'msg': 'training on 290760 raw words (148580 effective words) took 0.5s, 322218 effective words/s', 'datetime': '2023-05-16T02:32:07.012877', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'train'}\n",
      "2023-05-16 02:32:07,013 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=889, vector_size=100, alpha=0.025>', 'datetime': '2023-05-16T02:32:07.013477', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sentences, min_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vector_size\n",
    "-----------\n",
    "\n",
    "``vector_size`` is the number of dimensions (N) of the N-dimensional space that\n",
    "gensim Word2Vec maps the words onto.\n",
    "\n",
    "Bigger size values require more training data, but can lead to better (more\n",
    "accurate) models. Reasonable values are in the tens to hundreds.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 02:32:11,170 : INFO : collecting all words and their counts\n",
      "2023-05-16 02:32:11,172 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-05-16 02:32:11,254 : INFO : collected 6981 word types from a corpus of 58152 raw words and 300 sentences\n",
      "2023-05-16 02:32:11,255 : INFO : Creating a fresh vocabulary\n",
      "2023-05-16 02:32:11,263 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1750 unique words (25.07% of original 6981, drops 5231)', 'datetime': '2023-05-16T02:32:11.263931', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2023-05-16 02:32:11,264 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 49335 word corpus (84.84% of original 58152, drops 8817)', 'datetime': '2023-05-16T02:32:11.264789', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2023-05-16 02:32:11,278 : INFO : deleting the raw counts dictionary of 6981 items\n",
      "2023-05-16 02:32:11,279 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2023-05-16 02:32:11,279 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 35935.33721568072 word corpus (72.8%% of prior 49335)', 'datetime': '2023-05-16T02:32:11.279803', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2023-05-16 02:32:11,304 : INFO : estimated required memory for 1750 words and 200 dimensions: 3675000 bytes\n",
      "2023-05-16 02:32:11,305 : INFO : resetting layer weights\n",
      "2023-05-16 02:32:11,308 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-05-16T02:32:11.308148', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'build_vocab'}\n",
      "2023-05-16 02:32:11,309 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1750 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-05-16T02:32:11.309109', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'train'}\n",
      "2023-05-16 02:32:11,422 : INFO : EPOCH 0: training on 58152 raw words (35883 effective words) took 0.1s, 324176 effective words/s\n",
      "2023-05-16 02:32:11,522 : INFO : EPOCH 1: training on 58152 raw words (35900 effective words) took 0.1s, 363778 effective words/s\n",
      "2023-05-16 02:32:11,623 : INFO : EPOCH 2: training on 58152 raw words (35934 effective words) took 0.1s, 360830 effective words/s\n",
      "2023-05-16 02:32:11,708 : INFO : EPOCH 3: training on 58152 raw words (35883 effective words) took 0.1s, 429196 effective words/s\n",
      "2023-05-16 02:32:11,789 : INFO : EPOCH 4: training on 58152 raw words (35849 effective words) took 0.1s, 446903 effective words/s\n",
      "2023-05-16 02:32:11,790 : INFO : Word2Vec lifecycle event {'msg': 'training on 290760 raw words (179449 effective words) took 0.5s, 373441 effective words/s', 'datetime': '2023-05-16T02:32:11.790279', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'train'}\n",
      "2023-05-16 02:32:11,790 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1750, vector_size=200, alpha=0.025>', 'datetime': '2023-05-16T02:32:11.790614', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# The default value of vector_size is 100.\n",
    "model = gensim.models.Word2Vec(sentences, vector_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "workers\n",
    "-------\n",
    "\n",
    "``workers`` , the last of the major parameters (full list `here\n",
    "<http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec>`_)\n",
    "is for training parallelization, to speed up training:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 02:32:13,771 : INFO : collecting all words and their counts\n",
      "2023-05-16 02:32:13,775 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-05-16 02:32:13,868 : INFO : collected 6981 word types from a corpus of 58152 raw words and 300 sentences\n",
      "2023-05-16 02:32:13,869 : INFO : Creating a fresh vocabulary\n",
      "2023-05-16 02:32:13,877 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1750 unique words (25.07% of original 6981, drops 5231)', 'datetime': '2023-05-16T02:32:13.877250', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2023-05-16 02:32:13,877 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 49335 word corpus (84.84% of original 58152, drops 8817)', 'datetime': '2023-05-16T02:32:13.877936', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2023-05-16 02:32:13,890 : INFO : deleting the raw counts dictionary of 6981 items\n",
      "2023-05-16 02:32:13,890 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2023-05-16 02:32:13,891 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 35935.33721568072 word corpus (72.8%% of prior 49335)', 'datetime': '2023-05-16T02:32:13.891278', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'prepare_vocab'}\n",
      "2023-05-16 02:32:13,913 : INFO : estimated required memory for 1750 words and 100 dimensions: 2275000 bytes\n",
      "2023-05-16 02:32:13,914 : INFO : resetting layer weights\n",
      "2023-05-16 02:32:13,916 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-05-16T02:32:13.916043', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'build_vocab'}\n",
      "2023-05-16 02:32:13,916 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 1750 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-05-16T02:32:13.916888', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'train'}\n",
      "2023-05-16 02:32:14,011 : INFO : EPOCH 0: training on 58152 raw words (35974 effective words) took 0.1s, 387350 effective words/s\n",
      "2023-05-16 02:32:14,087 : INFO : EPOCH 1: training on 58152 raw words (35886 effective words) took 0.1s, 490393 effective words/s\n",
      "2023-05-16 02:32:14,158 : INFO : EPOCH 2: training on 58152 raw words (35885 effective words) took 0.1s, 518203 effective words/s\n",
      "2023-05-16 02:32:14,228 : INFO : EPOCH 3: training on 58152 raw words (35973 effective words) took 0.1s, 520806 effective words/s\n",
      "2023-05-16 02:32:14,305 : INFO : EPOCH 4: training on 58152 raw words (35921 effective words) took 0.0s, 832403 effective words/s\n",
      "2023-05-16 02:32:14,306 : INFO : Word2Vec lifecycle event {'msg': 'training on 290760 raw words (179639 effective words) took 0.4s, 462289 effective words/s', 'datetime': '2023-05-16T02:32:14.306071', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'train'}\n",
      "2023-05-16 02:32:14,306 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1750, vector_size=100, alpha=0.025>', 'datetime': '2023-05-16T02:32:14.306431', 'gensim': '4.2.0', 'python': '3.6.9 (default, Mar 10 2023, 16:46:00) \\n[GCC 8.4.0]', 'platform': 'Linux-5.4.0-148-generic-x86_64-with-Ubuntu-18.04-bionic', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# default value of workers=3 (tutorial says 1...)\n",
    "model = gensim.models.Word2Vec(sentences, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``workers`` parameter only has an effect if you have `Cython\n",
    "<http://cython.org/>`_ installed. Without Cython, you’ll only be able to use\n",
    "one core because of the `GIL\n",
    "<https://wiki.python.org/moin/GlobalInterpreterLock>`_ (and ``word2vec``\n",
    "training will be `miserably slow\n",
    "<http://rare-technologies.com/word2vec-in-python-part-two-optimizing/>`_\\ ).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory\n",
    "------\n",
    "\n",
    "At its core, ``word2vec`` model parameters are stored as matrices (NumPy\n",
    "arrays). Each array is **#vocabulary** (controlled by the ``min_count`` parameter)\n",
    "times **vector size** (the ``vector_size`` parameter) of floats (single precision aka 4 bytes).\n",
    "\n",
    "Three such matrices are held in RAM (work is underway to reduce that number\n",
    "to two, or even one). So if your input contains 100,000 unique words, and you\n",
    "asked for layer ``vector_size=200``\\ , the model will require approx.\n",
    "``100,000*200*4*3 bytes = ~229MB``.\n",
    "\n",
    "There’s a little extra memory needed for storing the vocabulary tree (100,000 words would\n",
    "take a few megabytes), but unless your words are extremely loooong strings, memory\n",
    "footprint will be dominated by the three matrices above.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating\n",
    "----------\n",
    "\n",
    "``Word2Vec`` training is an unsupervised task, there’s no good way to\n",
    "objectively evaluate the result. Evaluation depends on your end application.\n",
    "\n",
    "Google has released their testing set of about 20,000 syntactic and semantic\n",
    "test examples, following the “A is to B as C is to D” task. It is provided in\n",
    "the 'datasets' folder.\n",
    "\n",
    "For example a syntactic analogy of comparative type is ``bad:worse;good:?``.\n",
    "There are total of 9 types of syntactic comparisons in the dataset like\n",
    "plural nouns and nouns of opposite meaning.\n",
    "\n",
    "The semantic questions contain five types of semantic analogies, such as\n",
    "capital cities (``Paris:France;Tokyo:?``) or family members\n",
    "(``brother:sister;dad:?``).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim supports the same evaluation set, in exactly the same format:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 02:32:18,184 : INFO : Evaluating word analogies for top 300000 words in the model on /home/shinjini/.local/lib/python3.6/site-packages/gensim/test/test_data/questions-words.txt\n",
      "2023-05-16 02:32:18,194 : INFO : capital-common-countries: 0.0% (0/6)\n",
      "2023-05-16 02:32:18,223 : INFO : capital-world: 0.0% (0/2)\n",
      "2023-05-16 02:32:18,247 : INFO : family: 0.0% (0/6)\n",
      "2023-05-16 02:32:18,268 : INFO : gram3-comparative: 0.0% (0/20)\n",
      "2023-05-16 02:32:18,276 : INFO : gram4-superlative: 0.0% (0/12)\n",
      "2023-05-16 02:32:18,305 : INFO : gram5-present-participle: 0.0% (0/20)\n",
      "2023-05-16 02:32:18,333 : INFO : gram6-nationality-adjective: 0.0% (0/30)\n",
      "2023-05-16 02:32:18,358 : INFO : gram7-past-tense: 0.0% (0/20)\n",
      "2023-05-16 02:32:18,390 : INFO : gram8-plural: 0.0% (0/30)\n",
      "2023-05-16 02:32:18,398 : INFO : Quadruplets with out-of-vocabulary words: 99.3%\n",
      "2023-05-16 02:32:18,402 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
      "2023-05-16 02:32:18,406 : INFO : Total accuracy: 0.0% (0/146)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " [{'section': 'capital-common-countries',\n",
       "   'correct': [],\n",
       "   'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'),\n",
       "    ('CANBERRA', 'AUSTRALIA', 'PARIS', 'FRANCE'),\n",
       "    ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'),\n",
       "    ('KABUL', 'AFGHANISTAN', 'CANBERRA', 'AUSTRALIA'),\n",
       "    ('PARIS', 'FRANCE', 'CANBERRA', 'AUSTRALIA'),\n",
       "    ('PARIS', 'FRANCE', 'KABUL', 'AFGHANISTAN')]},\n",
       "  {'section': 'capital-world',\n",
       "   'correct': [],\n",
       "   'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'),\n",
       "    ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE')]},\n",
       "  {'section': 'currency', 'correct': [], 'incorrect': []},\n",
       "  {'section': 'city-in-state', 'correct': [], 'incorrect': []},\n",
       "  {'section': 'family',\n",
       "   'correct': [],\n",
       "   'incorrect': [('HE', 'SHE', 'HIS', 'HER'),\n",
       "    ('HE', 'SHE', 'MAN', 'WOMAN'),\n",
       "    ('HIS', 'HER', 'MAN', 'WOMAN'),\n",
       "    ('HIS', 'HER', 'HE', 'SHE'),\n",
       "    ('MAN', 'WOMAN', 'HE', 'SHE'),\n",
       "    ('MAN', 'WOMAN', 'HIS', 'HER')]},\n",
       "  {'section': 'gram1-adjective-to-adverb', 'correct': [], 'incorrect': []},\n",
       "  {'section': 'gram2-opposite', 'correct': [], 'incorrect': []},\n",
       "  {'section': 'gram3-comparative',\n",
       "   'correct': [],\n",
       "   'incorrect': [('GOOD', 'BETTER', 'GREAT', 'GREATER'),\n",
       "    ('GOOD', 'BETTER', 'LONG', 'LONGER'),\n",
       "    ('GOOD', 'BETTER', 'LOW', 'LOWER'),\n",
       "    ('GOOD', 'BETTER', 'SMALL', 'SMALLER'),\n",
       "    ('GREAT', 'GREATER', 'LONG', 'LONGER'),\n",
       "    ('GREAT', 'GREATER', 'LOW', 'LOWER'),\n",
       "    ('GREAT', 'GREATER', 'SMALL', 'SMALLER'),\n",
       "    ('GREAT', 'GREATER', 'GOOD', 'BETTER'),\n",
       "    ('LONG', 'LONGER', 'LOW', 'LOWER'),\n",
       "    ('LONG', 'LONGER', 'SMALL', 'SMALLER'),\n",
       "    ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n",
       "    ('LONG', 'LONGER', 'GREAT', 'GREATER'),\n",
       "    ('LOW', 'LOWER', 'SMALL', 'SMALLER'),\n",
       "    ('LOW', 'LOWER', 'GOOD', 'BETTER'),\n",
       "    ('LOW', 'LOWER', 'GREAT', 'GREATER'),\n",
       "    ('LOW', 'LOWER', 'LONG', 'LONGER'),\n",
       "    ('SMALL', 'SMALLER', 'GOOD', 'BETTER'),\n",
       "    ('SMALL', 'SMALLER', 'GREAT', 'GREATER'),\n",
       "    ('SMALL', 'SMALLER', 'LONG', 'LONGER'),\n",
       "    ('SMALL', 'SMALLER', 'LOW', 'LOWER')]},\n",
       "  {'section': 'gram4-superlative',\n",
       "   'correct': [],\n",
       "   'incorrect': [('BIG', 'BIGGEST', 'GOOD', 'BEST'),\n",
       "    ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),\n",
       "    ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),\n",
       "    ('GOOD', 'BEST', 'GREAT', 'GREATEST'),\n",
       "    ('GOOD', 'BEST', 'LARGE', 'LARGEST'),\n",
       "    ('GOOD', 'BEST', 'BIG', 'BIGGEST'),\n",
       "    ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),\n",
       "    ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),\n",
       "    ('GREAT', 'GREATEST', 'GOOD', 'BEST'),\n",
       "    ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),\n",
       "    ('LARGE', 'LARGEST', 'GOOD', 'BEST'),\n",
       "    ('LARGE', 'LARGEST', 'GREAT', 'GREATEST')]},\n",
       "  {'section': 'gram5-present-participle',\n",
       "   'correct': [],\n",
       "   'incorrect': [('GO', 'GOING', 'LOOK', 'LOOKING'),\n",
       "    ('GO', 'GOING', 'PLAY', 'PLAYING'),\n",
       "    ('GO', 'GOING', 'RUN', 'RUNNING'),\n",
       "    ('GO', 'GOING', 'SAY', 'SAYING'),\n",
       "    ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n",
       "    ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n",
       "    ('LOOK', 'LOOKING', 'SAY', 'SAYING'),\n",
       "    ('LOOK', 'LOOKING', 'GO', 'GOING'),\n",
       "    ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n",
       "    ('PLAY', 'PLAYING', 'SAY', 'SAYING'),\n",
       "    ('PLAY', 'PLAYING', 'GO', 'GOING'),\n",
       "    ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n",
       "    ('RUN', 'RUNNING', 'SAY', 'SAYING'),\n",
       "    ('RUN', 'RUNNING', 'GO', 'GOING'),\n",
       "    ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n",
       "    ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n",
       "    ('SAY', 'SAYING', 'GO', 'GOING'),\n",
       "    ('SAY', 'SAYING', 'LOOK', 'LOOKING'),\n",
       "    ('SAY', 'SAYING', 'PLAY', 'PLAYING'),\n",
       "    ('SAY', 'SAYING', 'RUN', 'RUNNING')]},\n",
       "  {'section': 'gram6-nationality-adjective',\n",
       "   'correct': [],\n",
       "   'incorrect': [('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),\n",
       "    ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),\n",
       "    ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),\n",
       "    ('AUSTRALIA', 'AUSTRALIAN', 'JAPAN', 'JAPANESE'),\n",
       "    ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),\n",
       "    ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),\n",
       "    ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),\n",
       "    ('FRANCE', 'FRENCH', 'JAPAN', 'JAPANESE'),\n",
       "    ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),\n",
       "    ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "    ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),\n",
       "    ('INDIA', 'INDIAN', 'JAPAN', 'JAPANESE'),\n",
       "    ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),\n",
       "    ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "    ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),\n",
       "    ('ISRAEL', 'ISRAELI', 'JAPAN', 'JAPANESE'),\n",
       "    ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),\n",
       "    ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "    ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),\n",
       "    ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),\n",
       "    ('JAPAN', 'JAPANESE', 'SWITZERLAND', 'SWISS'),\n",
       "    ('JAPAN', 'JAPANESE', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "    ('JAPAN', 'JAPANESE', 'FRANCE', 'FRENCH'),\n",
       "    ('JAPAN', 'JAPANESE', 'INDIA', 'INDIAN'),\n",
       "    ('JAPAN', 'JAPANESE', 'ISRAEL', 'ISRAELI'),\n",
       "    ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "    ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),\n",
       "    ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),\n",
       "    ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'),\n",
       "    ('SWITZERLAND', 'SWISS', 'JAPAN', 'JAPANESE')]},\n",
       "  {'section': 'gram7-past-tense',\n",
       "   'correct': [],\n",
       "   'incorrect': [('GOING', 'WENT', 'PAYING', 'PAID'),\n",
       "    ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n",
       "    ('GOING', 'WENT', 'SAYING', 'SAID'),\n",
       "    ('GOING', 'WENT', 'TAKING', 'TOOK'),\n",
       "    ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),\n",
       "    ('PAYING', 'PAID', 'SAYING', 'SAID'),\n",
       "    ('PAYING', 'PAID', 'TAKING', 'TOOK'),\n",
       "    ('PAYING', 'PAID', 'GOING', 'WENT'),\n",
       "    ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),\n",
       "    ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n",
       "    ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n",
       "    ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),\n",
       "    ('SAYING', 'SAID', 'TAKING', 'TOOK'),\n",
       "    ('SAYING', 'SAID', 'GOING', 'WENT'),\n",
       "    ('SAYING', 'SAID', 'PAYING', 'PAID'),\n",
       "    ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),\n",
       "    ('TAKING', 'TOOK', 'GOING', 'WENT'),\n",
       "    ('TAKING', 'TOOK', 'PAYING', 'PAID'),\n",
       "    ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n",
       "    ('TAKING', 'TOOK', 'SAYING', 'SAID')]},\n",
       "  {'section': 'gram8-plural',\n",
       "   'correct': [],\n",
       "   'incorrect': [('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),\n",
       "    ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),\n",
       "    ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),\n",
       "    ('BUILDING', 'BUILDINGS', 'ROAD', 'ROADS'),\n",
       "    ('BUILDING', 'BUILDINGS', 'WOMAN', 'WOMEN'),\n",
       "    ('CAR', 'CARS', 'CHILD', 'CHILDREN'),\n",
       "    ('CAR', 'CARS', 'MAN', 'MEN'),\n",
       "    ('CAR', 'CARS', 'ROAD', 'ROADS'),\n",
       "    ('CAR', 'CARS', 'WOMAN', 'WOMEN'),\n",
       "    ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),\n",
       "    ('CHILD', 'CHILDREN', 'MAN', 'MEN'),\n",
       "    ('CHILD', 'CHILDREN', 'ROAD', 'ROADS'),\n",
       "    ('CHILD', 'CHILDREN', 'WOMAN', 'WOMEN'),\n",
       "    ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),\n",
       "    ('CHILD', 'CHILDREN', 'CAR', 'CARS'),\n",
       "    ('MAN', 'MEN', 'ROAD', 'ROADS'),\n",
       "    ('MAN', 'MEN', 'WOMAN', 'WOMEN'),\n",
       "    ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),\n",
       "    ('MAN', 'MEN', 'CAR', 'CARS'),\n",
       "    ('MAN', 'MEN', 'CHILD', 'CHILDREN'),\n",
       "    ('ROAD', 'ROADS', 'WOMAN', 'WOMEN'),\n",
       "    ('ROAD', 'ROADS', 'BUILDING', 'BUILDINGS'),\n",
       "    ('ROAD', 'ROADS', 'CAR', 'CARS'),\n",
       "    ('ROAD', 'ROADS', 'CHILD', 'CHILDREN'),\n",
       "    ('ROAD', 'ROADS', 'MAN', 'MEN'),\n",
       "    ('WOMAN', 'WOMEN', 'BUILDING', 'BUILDINGS'),\n",
       "    ('WOMAN', 'WOMEN', 'CAR', 'CARS'),\n",
       "    ('WOMAN', 'WOMEN', 'CHILD', 'CHILDREN'),\n",
       "    ('WOMAN', 'WOMEN', 'MAN', 'MEN'),\n",
       "    ('WOMAN', 'WOMEN', 'ROAD', 'ROADS')]},\n",
       "  {'section': 'gram9-plural-verbs', 'correct': [], 'incorrect': []},\n",
       "  {'section': 'Total accuracy',\n",
       "   'correct': [],\n",
       "   'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'),\n",
       "    ('CANBERRA', 'AUSTRALIA', 'PARIS', 'FRANCE'),\n",
       "    ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'),\n",
       "    ('KABUL', 'AFGHANISTAN', 'CANBERRA', 'AUSTRALIA'),\n",
       "    ('PARIS', 'FRANCE', 'CANBERRA', 'AUSTRALIA'),\n",
       "    ('PARIS', 'FRANCE', 'KABUL', 'AFGHANISTAN'),\n",
       "    ('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'),\n",
       "    ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'),\n",
       "    ('HE', 'SHE', 'HIS', 'HER'),\n",
       "    ('HE', 'SHE', 'MAN', 'WOMAN'),\n",
       "    ('HIS', 'HER', 'MAN', 'WOMAN'),\n",
       "    ('HIS', 'HER', 'HE', 'SHE'),\n",
       "    ('MAN', 'WOMAN', 'HE', 'SHE'),\n",
       "    ('MAN', 'WOMAN', 'HIS', 'HER'),\n",
       "    ('GOOD', 'BETTER', 'GREAT', 'GREATER'),\n",
       "    ('GOOD', 'BETTER', 'LONG', 'LONGER'),\n",
       "    ('GOOD', 'BETTER', 'LOW', 'LOWER'),\n",
       "    ('GOOD', 'BETTER', 'SMALL', 'SMALLER'),\n",
       "    ('GREAT', 'GREATER', 'LONG', 'LONGER'),\n",
       "    ('GREAT', 'GREATER', 'LOW', 'LOWER'),\n",
       "    ('GREAT', 'GREATER', 'SMALL', 'SMALLER'),\n",
       "    ('GREAT', 'GREATER', 'GOOD', 'BETTER'),\n",
       "    ('LONG', 'LONGER', 'LOW', 'LOWER'),\n",
       "    ('LONG', 'LONGER', 'SMALL', 'SMALLER'),\n",
       "    ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n",
       "    ('LONG', 'LONGER', 'GREAT', 'GREATER'),\n",
       "    ('LOW', 'LOWER', 'SMALL', 'SMALLER'),\n",
       "    ('LOW', 'LOWER', 'GOOD', 'BETTER'),\n",
       "    ('LOW', 'LOWER', 'GREAT', 'GREATER'),\n",
       "    ('LOW', 'LOWER', 'LONG', 'LONGER'),\n",
       "    ('SMALL', 'SMALLER', 'GOOD', 'BETTER'),\n",
       "    ('SMALL', 'SMALLER', 'GREAT', 'GREATER'),\n",
       "    ('SMALL', 'SMALLER', 'LONG', 'LONGER'),\n",
       "    ('SMALL', 'SMALLER', 'LOW', 'LOWER'),\n",
       "    ('BIG', 'BIGGEST', 'GOOD', 'BEST'),\n",
       "    ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),\n",
       "    ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),\n",
       "    ('GOOD', 'BEST', 'GREAT', 'GREATEST'),\n",
       "    ('GOOD', 'BEST', 'LARGE', 'LARGEST'),\n",
       "    ('GOOD', 'BEST', 'BIG', 'BIGGEST'),\n",
       "    ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),\n",
       "    ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),\n",
       "    ('GREAT', 'GREATEST', 'GOOD', 'BEST'),\n",
       "    ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),\n",
       "    ('LARGE', 'LARGEST', 'GOOD', 'BEST'),\n",
       "    ('LARGE', 'LARGEST', 'GREAT', 'GREATEST'),\n",
       "    ('GO', 'GOING', 'LOOK', 'LOOKING'),\n",
       "    ('GO', 'GOING', 'PLAY', 'PLAYING'),\n",
       "    ('GO', 'GOING', 'RUN', 'RUNNING'),\n",
       "    ('GO', 'GOING', 'SAY', 'SAYING'),\n",
       "    ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n",
       "    ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n",
       "    ('LOOK', 'LOOKING', 'SAY', 'SAYING'),\n",
       "    ('LOOK', 'LOOKING', 'GO', 'GOING'),\n",
       "    ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n",
       "    ('PLAY', 'PLAYING', 'SAY', 'SAYING'),\n",
       "    ('PLAY', 'PLAYING', 'GO', 'GOING'),\n",
       "    ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n",
       "    ('RUN', 'RUNNING', 'SAY', 'SAYING'),\n",
       "    ('RUN', 'RUNNING', 'GO', 'GOING'),\n",
       "    ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n",
       "    ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n",
       "    ('SAY', 'SAYING', 'GO', 'GOING'),\n",
       "    ('SAY', 'SAYING', 'LOOK', 'LOOKING'),\n",
       "    ('SAY', 'SAYING', 'PLAY', 'PLAYING'),\n",
       "    ('SAY', 'SAYING', 'RUN', 'RUNNING'),\n",
       "    ('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),\n",
       "    ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),\n",
       "    ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),\n",
       "    ('AUSTRALIA', 'AUSTRALIAN', 'JAPAN', 'JAPANESE'),\n",
       "    ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),\n",
       "    ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),\n",
       "    ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),\n",
       "    ('FRANCE', 'FRENCH', 'JAPAN', 'JAPANESE'),\n",
       "    ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),\n",
       "    ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "    ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),\n",
       "    ('INDIA', 'INDIAN', 'JAPAN', 'JAPANESE'),\n",
       "    ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),\n",
       "    ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "    ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),\n",
       "    ('ISRAEL', 'ISRAELI', 'JAPAN', 'JAPANESE'),\n",
       "    ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),\n",
       "    ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "    ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),\n",
       "    ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),\n",
       "    ('JAPAN', 'JAPANESE', 'SWITZERLAND', 'SWISS'),\n",
       "    ('JAPAN', 'JAPANESE', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "    ('JAPAN', 'JAPANESE', 'FRANCE', 'FRENCH'),\n",
       "    ('JAPAN', 'JAPANESE', 'INDIA', 'INDIAN'),\n",
       "    ('JAPAN', 'JAPANESE', 'ISRAEL', 'ISRAELI'),\n",
       "    ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "    ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),\n",
       "    ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),\n",
       "    ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'),\n",
       "    ('SWITZERLAND', 'SWISS', 'JAPAN', 'JAPANESE'),\n",
       "    ('GOING', 'WENT', 'PAYING', 'PAID'),\n",
       "    ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n",
       "    ('GOING', 'WENT', 'SAYING', 'SAID'),\n",
       "    ('GOING', 'WENT', 'TAKING', 'TOOK'),\n",
       "    ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),\n",
       "    ('PAYING', 'PAID', 'SAYING', 'SAID'),\n",
       "    ('PAYING', 'PAID', 'TAKING', 'TOOK'),\n",
       "    ('PAYING', 'PAID', 'GOING', 'WENT'),\n",
       "    ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),\n",
       "    ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n",
       "    ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n",
       "    ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),\n",
       "    ('SAYING', 'SAID', 'TAKING', 'TOOK'),\n",
       "    ('SAYING', 'SAID', 'GOING', 'WENT'),\n",
       "    ('SAYING', 'SAID', 'PAYING', 'PAID'),\n",
       "    ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),\n",
       "    ('TAKING', 'TOOK', 'GOING', 'WENT'),\n",
       "    ('TAKING', 'TOOK', 'PAYING', 'PAID'),\n",
       "    ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n",
       "    ('TAKING', 'TOOK', 'SAYING', 'SAID'),\n",
       "    ('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),\n",
       "    ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),\n",
       "    ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),\n",
       "    ('BUILDING', 'BUILDINGS', 'ROAD', 'ROADS'),\n",
       "    ('BUILDING', 'BUILDINGS', 'WOMAN', 'WOMEN'),\n",
       "    ('CAR', 'CARS', 'CHILD', 'CHILDREN'),\n",
       "    ('CAR', 'CARS', 'MAN', 'MEN'),\n",
       "    ('CAR', 'CARS', 'ROAD', 'ROADS'),\n",
       "    ('CAR', 'CARS', 'WOMAN', 'WOMEN'),\n",
       "    ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),\n",
       "    ('CHILD', 'CHILDREN', 'MAN', 'MEN'),\n",
       "    ('CHILD', 'CHILDREN', 'ROAD', 'ROADS'),\n",
       "    ('CHILD', 'CHILDREN', 'WOMAN', 'WOMEN'),\n",
       "    ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),\n",
       "    ('CHILD', 'CHILDREN', 'CAR', 'CARS'),\n",
       "    ('MAN', 'MEN', 'ROAD', 'ROADS'),\n",
       "    ('MAN', 'MEN', 'WOMAN', 'WOMEN'),\n",
       "    ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),\n",
       "    ('MAN', 'MEN', 'CAR', 'CARS'),\n",
       "    ('MAN', 'MEN', 'CHILD', 'CHILDREN'),\n",
       "    ('ROAD', 'ROADS', 'WOMAN', 'WOMEN'),\n",
       "    ('ROAD', 'ROADS', 'BUILDING', 'BUILDINGS'),\n",
       "    ('ROAD', 'ROADS', 'CAR', 'CARS'),\n",
       "    ('ROAD', 'ROADS', 'CHILD', 'CHILDREN'),\n",
       "    ('ROAD', 'ROADS', 'MAN', 'MEN'),\n",
       "    ('WOMAN', 'WOMEN', 'BUILDING', 'BUILDINGS'),\n",
       "    ('WOMAN', 'WOMEN', 'CAR', 'CARS'),\n",
       "    ('WOMAN', 'WOMEN', 'CHILD', 'CHILDREN'),\n",
       "    ('WOMAN', 'WOMEN', 'MAN', 'MEN'),\n",
       "    ('WOMAN', 'WOMEN', 'ROAD', 'ROADS')]}])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.evaluate_word_analogies(datapath('questions-words.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ``evaluate_word_analogies`` method takes an `optional parameter\n",
    "<http://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_analogies>`_\n",
    "``restrict_vocab`` which limits which test examples are to be considered.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the December 2016 release of Gensim we added a better way to evaluate semantic similarity.\n",
    "\n",
    "By default it uses an academic dataset WS-353 but one can create a dataset\n",
    "specific to your business based on it. It contains word pairs together with\n",
    "human-assigned similarity judgments. It measures the relatedness or\n",
    "co-occurrence of two words. For example, 'coast' and 'shore' are very similar\n",
    "as they appear in the same context. At the same time 'clothes' and 'closet'\n",
    "are less similar because they are related but not interchangeable.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 02:35:07,497 : INFO : Skipping line #2 with OOV words: love\tsex\t6.77\n",
      "2023-05-16 02:35:07,500 : INFO : Skipping line #3 with OOV words: tiger\tcat\t7.35\n",
      "2023-05-16 02:35:07,501 : INFO : Skipping line #4 with OOV words: tiger\ttiger\t10.00\n",
      "2023-05-16 02:35:07,502 : INFO : Skipping line #5 with OOV words: book\tpaper\t7.46\n",
      "2023-05-16 02:35:07,503 : INFO : Skipping line #6 with OOV words: computer\tkeyboard\t7.62\n",
      "2023-05-16 02:35:07,504 : INFO : Skipping line #7 with OOV words: computer\tinternet\t7.58\n",
      "2023-05-16 02:35:07,505 : INFO : Skipping line #9 with OOV words: train\tcar\t6.31\n",
      "2023-05-16 02:35:07,506 : INFO : Skipping line #10 with OOV words: telephone\tcommunication\t7.50\n",
      "2023-05-16 02:35:07,508 : INFO : Skipping line #14 with OOV words: bread\tbutter\t6.19\n",
      "2023-05-16 02:35:07,509 : INFO : Skipping line #15 with OOV words: cucumber\tpotato\t5.92\n",
      "2023-05-16 02:35:07,510 : INFO : Skipping line #16 with OOV words: doctor\tnurse\t7.00\n",
      "2023-05-16 02:35:07,511 : INFO : Skipping line #18 with OOV words: student\tprofessor\t6.81\n",
      "2023-05-16 02:35:07,511 : INFO : Skipping line #19 with OOV words: smart\tstudent\t4.62\n",
      "2023-05-16 02:35:07,512 : INFO : Skipping line #20 with OOV words: smart\tstupid\t5.81\n",
      "2023-05-16 02:35:07,513 : INFO : Skipping line #21 with OOV words: company\tstock\t7.08\n",
      "2023-05-16 02:35:07,514 : INFO : Skipping line #22 with OOV words: stock\tmarket\t8.08\n",
      "2023-05-16 02:35:07,516 : INFO : Skipping line #23 with OOV words: stock\tphone\t1.62\n",
      "2023-05-16 02:35:07,517 : INFO : Skipping line #24 with OOV words: stock\tCD\t1.31\n",
      "2023-05-16 02:35:07,518 : INFO : Skipping line #25 with OOV words: stock\tjaguar\t0.92\n",
      "2023-05-16 02:35:07,519 : INFO : Skipping line #26 with OOV words: stock\tegg\t1.81\n",
      "2023-05-16 02:35:07,520 : INFO : Skipping line #27 with OOV words: fertility\tegg\t6.69\n",
      "2023-05-16 02:35:07,522 : INFO : Skipping line #28 with OOV words: stock\tlive\t3.73\n",
      "2023-05-16 02:35:07,523 : INFO : Skipping line #29 with OOV words: stock\tlife\t0.92\n",
      "2023-05-16 02:35:07,524 : INFO : Skipping line #30 with OOV words: book\tlibrary\t7.46\n",
      "2023-05-16 02:35:07,525 : INFO : Skipping line #32 with OOV words: wood\tforest\t7.73\n",
      "2023-05-16 02:35:07,525 : INFO : Skipping line #33 with OOV words: money\tcash\t9.15\n",
      "2023-05-16 02:35:07,526 : INFO : Skipping line #34 with OOV words: professor\tcucumber\t0.31\n",
      "2023-05-16 02:35:07,527 : INFO : Skipping line #35 with OOV words: king\tcabbage\t0.23\n",
      "2023-05-16 02:35:07,528 : INFO : Skipping line #36 with OOV words: king\tqueen\t8.58\n",
      "2023-05-16 02:35:07,529 : INFO : Skipping line #37 with OOV words: king\trook\t5.92\n",
      "2023-05-16 02:35:07,530 : INFO : Skipping line #38 with OOV words: bishop\trabbi\t6.69\n",
      "2023-05-16 02:35:07,530 : INFO : Skipping line #41 with OOV words: holy\tsex\t1.62\n",
      "2023-05-16 02:35:07,531 : INFO : Skipping line #42 with OOV words: fuck\tsex\t9.44\n",
      "2023-05-16 02:35:07,533 : INFO : Skipping line #43 with OOV words: Maradona\tfootball\t8.62\n",
      "2023-05-16 02:35:07,534 : INFO : Skipping line #44 with OOV words: football\tsoccer\t9.03\n",
      "2023-05-16 02:35:07,537 : INFO : Skipping line #45 with OOV words: football\tbasketball\t6.81\n",
      "2023-05-16 02:35:07,539 : INFO : Skipping line #46 with OOV words: football\ttennis\t6.63\n",
      "2023-05-16 02:35:07,540 : INFO : Skipping line #47 with OOV words: tennis\tracket\t7.56\n",
      "2023-05-16 02:35:07,541 : INFO : Skipping line #50 with OOV words: Arafat\tJackson\t2.50\n",
      "2023-05-16 02:35:07,557 : INFO : Skipping line #51 with OOV words: law\tlawyer\t8.38\n",
      "2023-05-16 02:35:07,558 : INFO : Skipping line #52 with OOV words: movie\tstar\t7.38\n",
      "2023-05-16 02:35:07,559 : INFO : Skipping line #53 with OOV words: movie\tpopcorn\t6.19\n",
      "2023-05-16 02:35:07,559 : INFO : Skipping line #54 with OOV words: movie\tcritic\t6.73\n",
      "2023-05-16 02:35:07,560 : INFO : Skipping line #55 with OOV words: movie\ttheater\t7.92\n",
      "2023-05-16 02:35:07,562 : INFO : Skipping line #56 with OOV words: physics\tproton\t8.12\n",
      "2023-05-16 02:35:07,565 : INFO : Skipping line #57 with OOV words: physics\tchemistry\t7.35\n",
      "2023-05-16 02:35:07,566 : INFO : Skipping line #58 with OOV words: space\tchemistry\t4.88\n",
      "2023-05-16 02:35:07,567 : INFO : Skipping line #59 with OOV words: alcohol\tchemistry\t5.54\n",
      "2023-05-16 02:35:07,568 : INFO : Skipping line #60 with OOV words: vodka\tgin\t8.46\n",
      "2023-05-16 02:35:07,569 : INFO : Skipping line #61 with OOV words: vodka\tbrandy\t8.13\n",
      "2023-05-16 02:35:07,570 : INFO : Skipping line #62 with OOV words: drink\tcar\t3.04\n",
      "2023-05-16 02:35:07,571 : INFO : Skipping line #63 with OOV words: drink\tear\t1.31\n",
      "2023-05-16 02:35:07,571 : INFO : Skipping line #64 with OOV words: drink\tmouth\t5.96\n",
      "2023-05-16 02:35:07,572 : INFO : Skipping line #65 with OOV words: drink\teat\t6.87\n",
      "2023-05-16 02:35:07,573 : INFO : Skipping line #66 with OOV words: baby\tmother\t7.85\n",
      "2023-05-16 02:35:07,574 : INFO : Skipping line #67 with OOV words: drink\tmother\t2.65\n",
      "2023-05-16 02:35:07,575 : INFO : Skipping line #68 with OOV words: car\tautomobile\t8.94\n",
      "2023-05-16 02:35:07,577 : INFO : Skipping line #69 with OOV words: gem\tjewel\t8.96\n",
      "2023-05-16 02:35:07,578 : INFO : Skipping line #70 with OOV words: journey\tvoyage\t9.29\n",
      "2023-05-16 02:35:07,578 : INFO : Skipping line #71 with OOV words: boy\tlad\t8.83\n",
      "2023-05-16 02:35:07,579 : INFO : Skipping line #72 with OOV words: coast\tshore\t9.10\n",
      "2023-05-16 02:35:07,579 : INFO : Skipping line #73 with OOV words: asylum\tmadhouse\t8.87\n",
      "2023-05-16 02:35:07,581 : INFO : Skipping line #74 with OOV words: magician\twizard\t9.02\n",
      "2023-05-16 02:35:07,582 : INFO : Skipping line #75 with OOV words: midday\tnoon\t9.29\n",
      "2023-05-16 02:35:07,583 : INFO : Skipping line #76 with OOV words: furnace\tstove\t8.79\n",
      "2023-05-16 02:35:07,584 : INFO : Skipping line #77 with OOV words: food\tfruit\t7.52\n",
      "2023-05-16 02:35:07,585 : INFO : Skipping line #78 with OOV words: bird\tcock\t7.10\n",
      "2023-05-16 02:35:07,585 : INFO : Skipping line #79 with OOV words: bird\tcrane\t7.38\n",
      "2023-05-16 02:35:07,588 : INFO : Skipping line #80 with OOV words: tool\timplement\t6.46\n",
      "2023-05-16 02:35:07,589 : INFO : Skipping line #81 with OOV words: brother\tmonk\t6.27\n",
      "2023-05-16 02:35:07,590 : INFO : Skipping line #82 with OOV words: crane\timplement\t2.69\n",
      "2023-05-16 02:35:07,591 : INFO : Skipping line #83 with OOV words: lad\tbrother\t4.46\n",
      "2023-05-16 02:35:07,592 : INFO : Skipping line #84 with OOV words: journey\tcar\t5.85\n",
      "2023-05-16 02:35:07,593 : INFO : Skipping line #85 with OOV words: monk\toracle\t5.00\n",
      "2023-05-16 02:35:07,594 : INFO : Skipping line #86 with OOV words: cemetery\twoodland\t2.08\n",
      "2023-05-16 02:35:07,595 : INFO : Skipping line #87 with OOV words: food\trooster\t4.42\n",
      "2023-05-16 02:35:07,595 : INFO : Skipping line #89 with OOV words: forest\tgraveyard\t1.85\n",
      "2023-05-16 02:35:07,596 : INFO : Skipping line #90 with OOV words: shore\twoodland\t3.08\n",
      "2023-05-16 02:35:07,598 : INFO : Skipping line #91 with OOV words: monk\tslave\t0.92\n",
      "2023-05-16 02:35:07,598 : INFO : Skipping line #92 with OOV words: coast\tforest\t3.15\n",
      "2023-05-16 02:35:07,600 : INFO : Skipping line #93 with OOV words: lad\twizard\t0.92\n",
      "2023-05-16 02:35:07,602 : INFO : Skipping line #94 with OOV words: chord\tsmile\t0.54\n",
      "2023-05-16 02:35:07,603 : INFO : Skipping line #95 with OOV words: glass\tmagician\t2.08\n",
      "2023-05-16 02:35:07,604 : INFO : Skipping line #96 with OOV words: noon\tstring\t0.54\n",
      "2023-05-16 02:35:07,605 : INFO : Skipping line #97 with OOV words: rooster\tvoyage\t0.62\n",
      "2023-05-16 02:35:07,606 : INFO : Skipping line #98 with OOV words: money\tdollar\t8.42\n",
      "2023-05-16 02:35:07,606 : INFO : Skipping line #99 with OOV words: money\tcash\t9.08\n",
      "2023-05-16 02:35:07,607 : INFO : Skipping line #100 with OOV words: money\tcurrency\t9.04\n",
      "2023-05-16 02:35:07,608 : INFO : Skipping line #101 with OOV words: money\twealth\t8.27\n",
      "2023-05-16 02:35:07,609 : INFO : Skipping line #103 with OOV words: money\tpossession\t7.29\n",
      "2023-05-16 02:35:07,612 : INFO : Skipping line #105 with OOV words: money\tdeposit\t7.73\n",
      "2023-05-16 02:35:07,613 : INFO : Skipping line #106 with OOV words: money\twithdrawal\t6.88\n",
      "2023-05-16 02:35:07,614 : INFO : Skipping line #107 with OOV words: money\tlaundering\t5.65\n",
      "2023-05-16 02:35:07,615 : INFO : Skipping line #109 with OOV words: tiger\tjaguar\t8.00\n",
      "2023-05-16 02:35:07,616 : INFO : Skipping line #110 with OOV words: tiger\tfeline\t8.00\n",
      "2023-05-16 02:35:07,618 : INFO : Skipping line #111 with OOV words: tiger\tcarnivore\t7.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 02:35:07,619 : INFO : Skipping line #112 with OOV words: tiger\tmammal\t6.85\n",
      "2023-05-16 02:35:07,620 : INFO : Skipping line #113 with OOV words: tiger\tanimal\t7.00\n",
      "2023-05-16 02:35:07,621 : INFO : Skipping line #114 with OOV words: tiger\torganism\t4.77\n",
      "2023-05-16 02:35:07,622 : INFO : Skipping line #115 with OOV words: tiger\tfauna\t5.62\n",
      "2023-05-16 02:35:07,624 : INFO : Skipping line #116 with OOV words: tiger\tzoo\t5.87\n",
      "2023-05-16 02:35:07,625 : INFO : Skipping line #117 with OOV words: psychology\tpsychiatry\t8.08\n",
      "2023-05-16 02:35:07,627 : INFO : Skipping line #118 with OOV words: psychology\tanxiety\t7.00\n",
      "2023-05-16 02:35:07,629 : INFO : Skipping line #119 with OOV words: psychology\tfear\t6.85\n",
      "2023-05-16 02:35:07,630 : INFO : Skipping line #120 with OOV words: psychology\tdepression\t7.42\n",
      "2023-05-16 02:35:07,631 : INFO : Skipping line #121 with OOV words: psychology\tclinic\t6.58\n",
      "2023-05-16 02:35:07,633 : INFO : Skipping line #122 with OOV words: psychology\tdoctor\t6.42\n",
      "2023-05-16 02:35:07,634 : INFO : Skipping line #123 with OOV words: psychology\tFreud\t8.21\n",
      "2023-05-16 02:35:07,636 : INFO : Skipping line #124 with OOV words: psychology\tmind\t7.69\n",
      "2023-05-16 02:35:07,637 : INFO : Skipping line #125 with OOV words: psychology\thealth\t7.23\n",
      "2023-05-16 02:35:07,638 : INFO : Skipping line #126 with OOV words: psychology\tscience\t6.71\n",
      "2023-05-16 02:35:07,639 : INFO : Skipping line #127 with OOV words: psychology\tdiscipline\t5.58\n",
      "2023-05-16 02:35:07,640 : INFO : Skipping line #128 with OOV words: psychology\tcognition\t7.48\n",
      "2023-05-16 02:35:07,642 : INFO : Skipping line #129 with OOV words: planet\tstar\t8.45\n",
      "2023-05-16 02:35:07,643 : INFO : Skipping line #130 with OOV words: planet\tconstellation\t8.06\n",
      "2023-05-16 02:35:07,644 : INFO : Skipping line #131 with OOV words: planet\tmoon\t8.08\n",
      "2023-05-16 02:35:07,644 : INFO : Skipping line #132 with OOV words: planet\tsun\t8.02\n",
      "2023-05-16 02:35:07,645 : INFO : Skipping line #133 with OOV words: planet\tgalaxy\t8.11\n",
      "2023-05-16 02:35:07,647 : INFO : Skipping line #134 with OOV words: planet\tspace\t7.92\n",
      "2023-05-16 02:35:07,648 : INFO : Skipping line #135 with OOV words: planet\tastronomer\t7.94\n",
      "2023-05-16 02:35:07,650 : INFO : Skipping line #136 with OOV words: precedent\texample\t5.85\n",
      "2023-05-16 02:35:07,652 : INFO : Skipping line #137 with OOV words: precedent\tinformation\t3.85\n",
      "2023-05-16 02:35:07,653 : INFO : Skipping line #138 with OOV words: precedent\tcognition\t2.81\n",
      "2023-05-16 02:35:07,654 : INFO : Skipping line #139 with OOV words: precedent\tlaw\t6.65\n",
      "2023-05-16 02:35:07,655 : INFO : Skipping line #140 with OOV words: precedent\tcollection\t2.50\n",
      "2023-05-16 02:35:07,657 : INFO : Skipping line #141 with OOV words: precedent\tgroup\t1.77\n",
      "2023-05-16 02:35:07,658 : INFO : Skipping line #142 with OOV words: precedent\tantecedent\t6.04\n",
      "2023-05-16 02:35:07,659 : INFO : Skipping line #143 with OOV words: cup\tcoffee\t6.58\n",
      "2023-05-16 02:35:07,660 : INFO : Skipping line #144 with OOV words: cup\ttableware\t6.85\n",
      "2023-05-16 02:35:07,662 : INFO : Skipping line #145 with OOV words: cup\tarticle\t2.40\n",
      "2023-05-16 02:35:07,663 : INFO : Skipping line #146 with OOV words: cup\tartifact\t2.92\n",
      "2023-05-16 02:35:07,664 : INFO : Skipping line #147 with OOV words: cup\tobject\t3.69\n",
      "2023-05-16 02:35:07,665 : INFO : Skipping line #148 with OOV words: cup\tentity\t2.15\n",
      "2023-05-16 02:35:07,667 : INFO : Skipping line #149 with OOV words: cup\tdrink\t7.25\n",
      "2023-05-16 02:35:07,668 : INFO : Skipping line #151 with OOV words: cup\tsubstance\t1.92\n",
      "2023-05-16 02:35:07,669 : INFO : Skipping line #152 with OOV words: cup\tliquid\t5.90\n",
      "2023-05-16 02:35:07,670 : INFO : Skipping line #153 with OOV words: jaguar\tcat\t7.42\n",
      "2023-05-16 02:35:07,671 : INFO : Skipping line #154 with OOV words: jaguar\tcar\t7.27\n",
      "2023-05-16 02:35:07,672 : INFO : Skipping line #157 with OOV words: energy\tlaboratory\t5.09\n",
      "2023-05-16 02:35:07,673 : INFO : Skipping line #158 with OOV words: computer\tlaboratory\t6.78\n",
      "2023-05-16 02:35:07,674 : INFO : Skipping line #159 with OOV words: weapon\tsecret\t6.06\n",
      "2023-05-16 02:35:07,675 : INFO : Skipping line #160 with OOV words: FBI\tfingerprint\t6.94\n",
      "2023-05-16 02:35:07,676 : INFO : Skipping line #161 with OOV words: FBI\tinvestigation\t8.31\n",
      "2023-05-16 02:35:07,677 : INFO : Skipping line #163 with OOV words: Mars\twater\t2.94\n",
      "2023-05-16 02:35:07,678 : INFO : Skipping line #164 with OOV words: Mars\tscientist\t5.63\n",
      "2023-05-16 02:35:07,679 : INFO : Skipping line #166 with OOV words: canyon\tlandscape\t7.53\n",
      "2023-05-16 02:35:07,679 : INFO : Skipping line #167 with OOV words: image\tsurface\t4.56\n",
      "2023-05-16 02:35:07,680 : INFO : Skipping line #168 with OOV words: discovery\tspace\t6.34\n",
      "2023-05-16 02:35:07,681 : INFO : Skipping line #169 with OOV words: water\tseepage\t6.56\n",
      "2023-05-16 02:35:07,682 : INFO : Skipping line #170 with OOV words: sign\trecess\t2.38\n",
      "2023-05-16 02:35:07,683 : INFO : Skipping line #172 with OOV words: mile\tkilometer\t8.66\n",
      "2023-05-16 02:35:07,684 : INFO : Skipping line #173 with OOV words: computer\tnews\t4.47\n",
      "2023-05-16 02:35:07,688 : INFO : Skipping line #174 with OOV words: territory\tsurface\t5.34\n",
      "2023-05-16 02:35:07,689 : INFO : Skipping line #175 with OOV words: atmosphere\tlandscape\t3.69\n",
      "2023-05-16 02:35:07,690 : INFO : Skipping line #176 with OOV words: president\tmedal\t3.00\n",
      "2023-05-16 02:35:07,691 : INFO : Skipping line #179 with OOV words: skin\teye\t6.22\n",
      "2023-05-16 02:35:07,692 : INFO : Skipping line #181 with OOV words: theater\thistory\t3.91\n",
      "2023-05-16 02:35:07,693 : INFO : Skipping line #182 with OOV words: volunteer\tmotto\t2.56\n",
      "2023-05-16 02:35:07,694 : INFO : Skipping line #183 with OOV words: prejudice\trecognition\t3.00\n",
      "2023-05-16 02:35:07,695 : INFO : Skipping line #184 with OOV words: decoration\tvalor\t5.63\n",
      "2023-05-16 02:35:07,701 : INFO : Skipping line #185 with OOV words: century\tyear\t7.59\n",
      "2023-05-16 02:35:07,703 : INFO : Skipping line #186 with OOV words: century\tnation\t3.16\n",
      "2023-05-16 02:35:07,704 : INFO : Skipping line #187 with OOV words: delay\tracism\t1.19\n",
      "2023-05-16 02:35:07,706 : INFO : Skipping line #191 with OOV words: minority\tpeace\t3.69\n",
      "2023-05-16 02:35:07,707 : INFO : Skipping line #194 with OOV words: deployment\tdeparture\t4.25\n",
      "2023-05-16 02:35:07,708 : INFO : Skipping line #195 with OOV words: deployment\twithdrawal\t5.88\n",
      "2023-05-16 02:35:07,710 : INFO : Skipping line #197 with OOV words: announcement\tnews\t7.56\n",
      "2023-05-16 02:35:07,712 : INFO : Skipping line #198 with OOV words: announcement\teffort\t2.75\n",
      "2023-05-16 02:35:07,714 : INFO : Skipping line #199 with OOV words: stroke\thospital\t7.03\n",
      "2023-05-16 02:35:07,716 : INFO : Skipping line #200 with OOV words: disability\tdeath\t5.47\n",
      "2023-05-16 02:35:07,718 : INFO : Skipping line #201 with OOV words: victim\temergency\t6.47\n",
      "2023-05-16 02:35:07,719 : INFO : Skipping line #203 with OOV words: journal\tassociation\t4.97\n",
      "2023-05-16 02:35:07,721 : INFO : Skipping line #205 with OOV words: doctor\tliability\t5.19\n",
      "2023-05-16 02:35:07,723 : INFO : Skipping line #206 with OOV words: liability\tinsurance\t7.03\n",
      "2023-05-16 02:35:07,724 : INFO : Skipping line #207 with OOV words: school\tcenter\t3.44\n",
      "2023-05-16 02:35:07,725 : INFO : Skipping line #208 with OOV words: reason\thypertension\t2.31\n",
      "2023-05-16 02:35:07,727 : INFO : Skipping line #209 with OOV words: reason\tcriterion\t5.91\n",
      "2023-05-16 02:35:07,728 : INFO : Skipping line #210 with OOV words: hundred\tpercent\t7.38\n",
      "2023-05-16 02:35:07,730 : INFO : Skipping line #211 with OOV words: Harvard\tYale\t8.13\n",
      "2023-05-16 02:35:07,731 : INFO : Skipping line #212 with OOV words: hospital\tinfrastructure\t4.63\n",
      "2023-05-16 02:35:07,732 : INFO : Skipping line #213 with OOV words: death\trow\t5.25\n",
      "2023-05-16 02:35:07,736 : INFO : Skipping line #214 with OOV words: death\tinmate\t5.03\n",
      "2023-05-16 02:35:07,737 : INFO : Skipping line #215 with OOV words: lawyer\tevidence\t6.69\n",
      "2023-05-16 02:35:07,738 : INFO : Skipping line #218 with OOV words: word\tsimilarity\t4.75\n",
      "2023-05-16 02:35:07,740 : INFO : Skipping line #219 with OOV words: board\trecommendation\t4.47\n",
      "2023-05-16 02:35:07,741 : INFO : Skipping line #221 with OOV words: OPEC\tcountry\t5.63\n",
      "2023-05-16 02:35:07,743 : INFO : Skipping line #222 with OOV words: peace\tatmosphere\t3.69\n",
      "2023-05-16 02:35:07,744 : INFO : Skipping line #224 with OOV words: territory\tkilometer\t5.28\n",
      "2023-05-16 02:35:07,744 : INFO : Skipping line #226 with OOV words: competition\tprice\t6.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 02:35:07,745 : INFO : Skipping line #227 with OOV words: consumer\tconfidence\t4.13\n",
      "2023-05-16 02:35:07,746 : INFO : Skipping line #228 with OOV words: consumer\tenergy\t4.75\n",
      "2023-05-16 02:35:07,747 : INFO : Skipping line #231 with OOV words: credit\tcard\t8.06\n",
      "2023-05-16 02:35:07,750 : INFO : Skipping line #233 with OOV words: hotel\treservation\t8.03\n",
      "2023-05-16 02:35:07,751 : INFO : Skipping line #234 with OOV words: grocery\tmoney\t5.94\n",
      "2023-05-16 02:35:07,752 : INFO : Skipping line #235 with OOV words: registration\tarrangement\t6.00\n",
      "2023-05-16 02:35:07,753 : INFO : Skipping line #236 with OOV words: arrangement\taccommodation\t5.41\n",
      "2023-05-16 02:35:07,755 : INFO : Skipping line #238 with OOV words: type\tkind\t8.97\n",
      "2023-05-16 02:35:07,756 : INFO : Skipping line #239 with OOV words: arrival\thotel\t6.00\n",
      "2023-05-16 02:35:07,757 : INFO : Skipping line #240 with OOV words: bed\tcloset\t6.72\n",
      "2023-05-16 02:35:07,758 : INFO : Skipping line #241 with OOV words: closet\tclothes\t8.00\n",
      "2023-05-16 02:35:07,759 : INFO : Skipping line #242 with OOV words: situation\tconclusion\t4.81\n",
      "2023-05-16 02:35:07,761 : INFO : Skipping line #243 with OOV words: situation\tisolation\t3.88\n",
      "2023-05-16 02:35:07,766 : INFO : Skipping line #244 with OOV words: impartiality\tinterest\t5.16\n",
      "2023-05-16 02:35:07,768 : INFO : Skipping line #245 with OOV words: direction\tcombination\t2.25\n",
      "2023-05-16 02:35:07,771 : INFO : Skipping line #246 with OOV words: street\tplace\t6.44\n",
      "2023-05-16 02:35:07,772 : INFO : Skipping line #247 with OOV words: street\tavenue\t8.88\n",
      "2023-05-16 02:35:07,774 : INFO : Skipping line #248 with OOV words: street\tblock\t6.88\n",
      "2023-05-16 02:35:07,776 : INFO : Skipping line #249 with OOV words: street\tchildren\t4.94\n",
      "2023-05-16 02:35:07,777 : INFO : Skipping line #250 with OOV words: listing\tproximity\t2.56\n",
      "2023-05-16 02:35:07,779 : INFO : Skipping line #251 with OOV words: listing\tcategory\t6.38\n",
      "2023-05-16 02:35:07,780 : INFO : Skipping line #252 with OOV words: cell\tphone\t7.81\n",
      "2023-05-16 02:35:07,781 : INFO : Skipping line #253 with OOV words: production\thike\t1.75\n",
      "2023-05-16 02:35:07,783 : INFO : Skipping line #254 with OOV words: benchmark\tindex\t4.25\n",
      "2023-05-16 02:35:07,784 : INFO : Skipping line #256 with OOV words: media\tgain\t2.88\n",
      "2023-05-16 02:35:07,785 : INFO : Skipping line #257 with OOV words: dividend\tpayment\t7.63\n",
      "2023-05-16 02:35:07,786 : INFO : Skipping line #258 with OOV words: dividend\tcalculation\t6.48\n",
      "2023-05-16 02:35:07,788 : INFO : Skipping line #259 with OOV words: calculation\tcomputation\t8.44\n",
      "2023-05-16 02:35:07,789 : INFO : Skipping line #260 with OOV words: currency\tmarket\t7.50\n",
      "2023-05-16 02:35:07,790 : INFO : Skipping line #261 with OOV words: OPEC\toil\t8.59\n",
      "2023-05-16 02:35:07,791 : INFO : Skipping line #262 with OOV words: oil\tstock\t6.34\n",
      "2023-05-16 02:35:07,792 : INFO : Skipping line #263 with OOV words: announcement\tproduction\t3.38\n",
      "2023-05-16 02:35:07,793 : INFO : Skipping line #264 with OOV words: announcement\twarning\t6.00\n",
      "2023-05-16 02:35:07,794 : INFO : Skipping line #265 with OOV words: profit\twarning\t3.88\n",
      "2023-05-16 02:35:07,795 : INFO : Skipping line #266 with OOV words: profit\tloss\t7.63\n",
      "2023-05-16 02:35:07,795 : INFO : Skipping line #267 with OOV words: dollar\tyen\t7.78\n",
      "2023-05-16 02:35:07,796 : INFO : Skipping line #268 with OOV words: dollar\tbuck\t9.22\n",
      "2023-05-16 02:35:07,798 : INFO : Skipping line #269 with OOV words: dollar\tprofit\t7.38\n",
      "2023-05-16 02:35:07,800 : INFO : Skipping line #270 with OOV words: dollar\tloss\t6.09\n",
      "2023-05-16 02:35:07,801 : INFO : Skipping line #271 with OOV words: computer\tsoftware\t8.50\n",
      "2023-05-16 02:35:07,802 : INFO : Skipping line #272 with OOV words: network\thardware\t8.31\n",
      "2023-05-16 02:35:07,804 : INFO : Skipping line #273 with OOV words: phone\tequipment\t7.13\n",
      "2023-05-16 02:35:07,805 : INFO : Skipping line #274 with OOV words: equipment\tmaker\t5.91\n",
      "2023-05-16 02:35:07,806 : INFO : Skipping line #275 with OOV words: luxury\tcar\t6.47\n",
      "2023-05-16 02:35:07,806 : INFO : Skipping line #277 with OOV words: report\tgain\t3.63\n",
      "2023-05-16 02:35:07,807 : INFO : Skipping line #278 with OOV words: investor\tearning\t7.13\n",
      "2023-05-16 02:35:07,808 : INFO : Skipping line #279 with OOV words: liquid\twater\t7.89\n",
      "2023-05-16 02:35:07,808 : INFO : Skipping line #280 with OOV words: baseball\tseason\t5.97\n",
      "2023-05-16 02:35:07,809 : INFO : Skipping line #283 with OOV words: marathon\tsprint\t7.47\n",
      "2023-05-16 02:35:07,810 : INFO : Skipping line #285 with OOV words: game\tdefeat\t6.97\n",
      "2023-05-16 02:35:07,811 : INFO : Skipping line #287 with OOV words: seafood\tsea\t7.47\n",
      "2023-05-16 02:35:07,813 : INFO : Skipping line #288 with OOV words: seafood\tfood\t8.34\n",
      "2023-05-16 02:35:07,814 : INFO : Skipping line #289 with OOV words: seafood\tlobster\t8.70\n",
      "2023-05-16 02:35:07,815 : INFO : Skipping line #290 with OOV words: lobster\tfood\t7.81\n",
      "2023-05-16 02:35:07,816 : INFO : Skipping line #291 with OOV words: lobster\twine\t5.70\n",
      "2023-05-16 02:35:07,817 : INFO : Skipping line #292 with OOV words: food\tpreparation\t6.22\n",
      "2023-05-16 02:35:07,819 : INFO : Skipping line #293 with OOV words: video\tarchive\t6.34\n",
      "2023-05-16 02:35:07,820 : INFO : Skipping line #298 with OOV words: championship\ttournament\t8.36\n",
      "2023-05-16 02:35:07,821 : INFO : Skipping line #299 with OOV words: fighting\tdefeating\t7.41\n",
      "2023-05-16 02:35:07,822 : INFO : Skipping line #301 with OOV words: day\tsummer\t3.94\n",
      "2023-05-16 02:35:07,823 : INFO : Skipping line #302 with OOV words: summer\tdrought\t7.16\n",
      "2023-05-16 02:35:07,825 : INFO : Skipping line #303 with OOV words: summer\tnature\t5.63\n",
      "2023-05-16 02:35:07,827 : INFO : Skipping line #304 with OOV words: day\tdawn\t7.53\n",
      "2023-05-16 02:35:07,829 : INFO : Skipping line #305 with OOV words: nature\tenvironment\t8.31\n",
      "2023-05-16 02:35:07,830 : INFO : Skipping line #306 with OOV words: environment\tecology\t8.81\n",
      "2023-05-16 02:35:07,831 : INFO : Skipping line #307 with OOV words: nature\tman\t6.25\n",
      "2023-05-16 02:35:07,832 : INFO : Skipping line #311 with OOV words: soap\topera\t7.94\n",
      "2023-05-16 02:35:07,834 : INFO : Skipping line #312 with OOV words: opera\tperformance\t6.88\n",
      "2023-05-16 02:35:07,836 : INFO : Skipping line #313 with OOV words: life\tlesson\t5.94\n",
      "2023-05-16 02:35:07,838 : INFO : Skipping line #315 with OOV words: production\tcrew\t6.25\n",
      "2023-05-16 02:35:07,839 : INFO : Skipping line #316 with OOV words: television\tfilm\t7.72\n",
      "2023-05-16 02:35:07,839 : INFO : Skipping line #317 with OOV words: lover\tquarrel\t6.19\n",
      "2023-05-16 02:35:07,841 : INFO : Skipping line #318 with OOV words: viewer\tserial\t2.97\n",
      "2023-05-16 02:35:07,841 : INFO : Skipping line #319 with OOV words: possibility\tgirl\t1.94\n",
      "2023-05-16 02:35:07,842 : INFO : Skipping line #320 with OOV words: population\tdevelopment\t3.75\n",
      "2023-05-16 02:35:07,843 : INFO : Skipping line #321 with OOV words: morality\timportance\t3.31\n",
      "2023-05-16 02:35:07,844 : INFO : Skipping line #322 with OOV words: morality\tmarriage\t3.69\n",
      "2023-05-16 02:35:07,845 : INFO : Skipping line #323 with OOV words: Mexico\tBrazil\t7.44\n",
      "2023-05-16 02:35:07,847 : INFO : Skipping line #324 with OOV words: gender\tequality\t6.41\n",
      "2023-05-16 02:35:07,848 : INFO : Skipping line #325 with OOV words: change\tattitude\t5.44\n",
      "2023-05-16 02:35:07,849 : INFO : Skipping line #327 with OOV words: opera\tindustry\t2.63\n",
      "2023-05-16 02:35:07,850 : INFO : Skipping line #328 with OOV words: sugar\tapproach\t0.88\n",
      "2023-05-16 02:35:07,851 : INFO : Skipping line #329 with OOV words: practice\tinstitution\t3.19\n",
      "2023-05-16 02:35:07,852 : INFO : Skipping line #330 with OOV words: ministry\tculture\t4.69\n",
      "2023-05-16 02:35:07,857 : INFO : Skipping line #331 with OOV words: problem\tchallenge\t6.75\n",
      "2023-05-16 02:35:07,858 : INFO : Skipping line #332 with OOV words: size\tprominence\t5.31\n",
      "2023-05-16 02:35:07,864 : INFO : Skipping line #333 with OOV words: country\tcitizen\t7.31\n",
      "2023-05-16 02:35:07,865 : INFO : Skipping line #334 with OOV words: planet\tpeople\t5.75\n",
      "2023-05-16 02:35:07,866 : INFO : Skipping line #335 with OOV words: development\tissue\t3.97\n",
      "2023-05-16 02:35:07,867 : INFO : Skipping line #336 with OOV words: experience\tmusic\t3.47\n",
      "2023-05-16 02:35:07,873 : INFO : Skipping line #337 with OOV words: music\tproject\t3.63\n",
      "2023-05-16 02:35:07,875 : INFO : Skipping line #338 with OOV words: glass\tmetal\t5.56\n",
      "2023-05-16 02:35:07,876 : INFO : Skipping line #339 with OOV words: aluminum\tmetal\t7.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 02:35:07,877 : INFO : Skipping line #340 with OOV words: chance\tcredibility\t3.88\n",
      "2023-05-16 02:35:07,878 : INFO : Skipping line #341 with OOV words: exhibit\tmemorabilia\t5.31\n",
      "2023-05-16 02:35:07,880 : INFO : Skipping line #342 with OOV words: concert\tvirtuoso\t6.81\n",
      "2023-05-16 02:35:07,881 : INFO : Skipping line #343 with OOV words: rock\tjazz\t7.59\n",
      "2023-05-16 02:35:07,883 : INFO : Skipping line #344 with OOV words: museum\ttheater\t7.19\n",
      "2023-05-16 02:35:07,885 : INFO : Skipping line #345 with OOV words: observation\tarchitecture\t4.38\n",
      "2023-05-16 02:35:07,887 : INFO : Skipping line #347 with OOV words: preservation\tworld\t6.19\n",
      "2023-05-16 02:35:07,888 : INFO : Skipping line #348 with OOV words: admission\tticket\t7.69\n",
      "2023-05-16 02:35:07,889 : INFO : Skipping line #349 with OOV words: shower\tthunderstorm\t6.31\n",
      "2023-05-16 02:35:07,890 : INFO : Skipping line #350 with OOV words: shower\tflood\t6.03\n",
      "2023-05-16 02:35:07,891 : INFO : Skipping line #354 with OOV words: architecture\tcentury\t3.78\n",
      "2023-05-16 02:35:07,898 : INFO : Pearson correlation coefficient against /home/shinjini/.local/lib/python3.6/site-packages/gensim/test/test_data/wordsim353.tsv: 0.1811\n",
      "2023-05-16 02:35:07,900 : INFO : Spearman rank-order correlation coefficient against /home/shinjini/.local/lib/python3.6/site-packages/gensim/test/test_data/wordsim353.tsv: 0.1425\n",
      "2023-05-16 02:35:07,901 : INFO : Pairs with unknown words ratio: 83.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.18114660097019084, 0.16601323581929714),\n",
       " SpearmanrResult(correlation=0.14247603256008165, pvalue=0.2775008997383203),\n",
       " 83.0028328611898)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. Important::\n",
    "  Good performance on Google's or WS-353 test set doesn’t mean word2vec will\n",
    "  work well in your application, or vice versa. It’s always best to evaluate\n",
    "  directly on your intended task. For an example of how to use word2vec in a\n",
    "  classifier pipeline, see this `tutorial\n",
    "  <https://github.com/RaRe-Technologies/movie-plots-by-genre>`_.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online training / Resuming training\n",
    "-----------------------------------\n",
    "\n",
    "Advanced users can load a model and continue training it with more sentences\n",
    "and `new vocabulary words <online_w2v_tutorial.ipynb>`_:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(temporary_filepath)\n",
    "more_sentences = [\n",
    "    ['Advanced', 'users', 'can', 'load', 'a', 'model',\n",
    "     'and', 'continue', 'training', 'it', 'with', 'more', 'sentences'],\n",
    "]\n",
    "model.build_vocab(more_sentences, update=True)\n",
    "model.train(more_sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# cleaning up temporary file\n",
    "import os\n",
    "os.remove(temporary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to tweak the ``total_words`` parameter to ``train()``,\n",
    "depending on what learning rate decay you want to simulate.\n",
    "\n",
    "Note that it’s not possible to resume training with models generated by the C\n",
    "tool, ``KeyedVectors.load_word2vec_format()``. You can still use them for\n",
    "querying/similarity, but information vital for training (the vocab tree) is\n",
    "missing there.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loss Computation\n",
    "-------------------------\n",
    "\n",
    "The parameter ``compute_loss`` can be used to toggle computation of loss\n",
    "while training the Word2Vec model. The computed loss is stored in the model\n",
    "attribute ``running_training_loss`` and can be retrieved using the function\n",
    "``get_latest_training_loss`` as follows :\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating and training the Word2Vec model\n",
    "model_with_loss = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    min_count=1,\n",
    "    compute_loss=True,\n",
    "    hs=0,\n",
    "    sg=1,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# getting the training loss value\n",
    "training_loss = model_with_loss.get_latest_training_loss()\n",
    "print(training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarks\n",
    "----------\n",
    "\n",
    "Let's run some benchmarks to see effect of the training loss computation code\n",
    "on training time.\n",
    "\n",
    "We'll use the following data for the benchmarks:\n",
    "\n",
    "#. Lee Background corpus: included in gensim's test data\n",
    "#. Text8 corpus.  To demonstrate the effect of corpus size, we'll look at the\n",
    "   first 1MB, 10MB, 50MB of the corpus, as well as the entire thing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "import gensim.models.word2vec\n",
    "import gensim.downloader as api\n",
    "import smart_open\n",
    "\n",
    "\n",
    "def head(path, size):\n",
    "    with smart_open.open(path) as fin:\n",
    "        return io.StringIO(fin.read(size))\n",
    "\n",
    "\n",
    "def generate_input_data():\n",
    "    lee_path = datapath('lee_background.cor')\n",
    "    ls = gensim.models.word2vec.LineSentence(lee_path)\n",
    "    ls.name = '25kB'\n",
    "    yield ls\n",
    "\n",
    "    text8_path = api.load('text8').fn\n",
    "    labels = ('1MB', '10MB', '50MB', '100MB')\n",
    "    sizes = (1024 ** 2, 10 * 1024 ** 2, 50 * 1024 ** 2, 100 * 1024 ** 2)\n",
    "    for l, s in zip(labels, sizes):\n",
    "        ls = gensim.models.word2vec.LineSentence(head(text8_path, s))\n",
    "        ls.name = l\n",
    "        yield ls\n",
    "\n",
    "\n",
    "input_data = list(generate_input_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare the training time taken for different combinations of input\n",
    "data and model training parameters like ``hs`` and ``sg``.\n",
    "\n",
    "For each combination, we repeat the test several times to obtain the mean and\n",
    "standard deviation of the test duration.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily reduce logging verbosity\n",
    "logging.root.level = logging.ERROR\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_time_values = []\n",
    "seed_val = 42\n",
    "sg_values = [0, 1]\n",
    "hs_values = [0, 1]\n",
    "\n",
    "fast = True\n",
    "if fast:\n",
    "    input_data_subset = input_data[:3]\n",
    "else:\n",
    "    input_data_subset = input_data\n",
    "\n",
    "\n",
    "for data in input_data_subset:\n",
    "    for sg_val in sg_values:\n",
    "        for hs_val in hs_values:\n",
    "            for loss_flag in [True, False]:\n",
    "                time_taken_list = []\n",
    "                for i in range(3):\n",
    "                    start_time = time.time()\n",
    "                    w2v_model = gensim.models.Word2Vec(\n",
    "                        data,\n",
    "                        compute_loss=loss_flag,\n",
    "                        sg=sg_val,\n",
    "                        hs=hs_val,\n",
    "                        seed=seed_val,\n",
    "                    )\n",
    "                    time_taken_list.append(time.time() - start_time)\n",
    "\n",
    "                time_taken_list = np.array(time_taken_list)\n",
    "                time_mean = np.mean(time_taken_list)\n",
    "                time_std = np.std(time_taken_list)\n",
    "\n",
    "                model_result = {\n",
    "                    'train_data': data.name,\n",
    "                    'compute_loss': loss_flag,\n",
    "                    'sg': sg_val,\n",
    "                    'hs': hs_val,\n",
    "                    'train_time_mean': time_mean,\n",
    "                    'train_time_std': time_std,\n",
    "                }\n",
    "                print(\"Word2vec model #%i: %s\" % (len(train_time_values), model_result))\n",
    "                train_time_values.append(model_result)\n",
    "\n",
    "train_times_table = pd.DataFrame(train_time_values)\n",
    "train_times_table = train_times_table.sort_values(\n",
    "    by=['train_data', 'sg', 'hs', 'compute_loss'],\n",
    "    ascending=[False, False, True, False],\n",
    ")\n",
    "print(train_times_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising Word Embeddings\n",
    "---------------------------\n",
    "\n",
    "The word embeddings made by the model can be visualised by reducing\n",
    "dimensionality of the words to 2 dimensions using tSNE.\n",
    "\n",
    "Visualisations can be used to notice semantic and syntactic trends in the data.\n",
    "\n",
    "Example:\n",
    "\n",
    "* Semantic: words like cat, dog, cow, etc. have a tendency to lie close by\n",
    "* Syntactic: words like run, running or cut, cutting lie close together.\n",
    "\n",
    "Vector relations like vKing - vMan = vQueen - vWoman can also be noticed.\n",
    "\n",
    ".. Important::\n",
    "  The model used for the visualisation is trained on a small corpus. Thus\n",
    "  some of the relations might not be so clear.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    # extract the words & their vectors, as numpy arrays\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\n",
    "\n",
    "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
    "    from plotly.offline import init_notebook_mode, iplot, plot\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
    "    data = [trace]\n",
    "\n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')\n",
    "\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    #\n",
    "    # Label randomly subsampled 25 data points\n",
    "    #\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "except Exception:\n",
    "    plot_function = plot_with_matplotlib\n",
    "else:\n",
    "    plot_function = plot_with_plotly\n",
    "\n",
    "plot_function(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "----------\n",
    "\n",
    "In this tutorial we learned how to train word2vec models on your custom data\n",
    "and also how to evaluate it. Hope that you too will find this popular tool\n",
    "useful in your Machine Learning tasks!\n",
    "\n",
    "Links\n",
    "-----\n",
    "\n",
    "- API docs: :py:mod:`gensim.models.word2vec`\n",
    "- `Original C toolkit and word2vec papers by Google <https://code.google.com/archive/p/word2vec/>`_.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
