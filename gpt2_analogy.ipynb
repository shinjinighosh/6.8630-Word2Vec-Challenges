{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT 2 - Analogies with Learned Embeddings\n",
    "---\n",
    "\n",
    "This document explores using the learned embeddings layer of the GPT 2 language model to perform analogy analysis, similar to the approach used for Word2Vec. The reason for exploring this analysis using GPT 2 rather than other learned embeddings like BERT is in response to the rise in prevalence of ChatGPT in modern day use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theng/miniconda3/envs/textgen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/theng/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/modeling_utils.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  with safe_open(checkpoint_file, framework=\"pt\") as f:\n",
      "/home/theng/miniconda3/envs/textgen/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/theng/miniconda3/envs/textgen/lib/python3.10/site-packages/torch/storage.py:899: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = cls(wrap_storage=untyped_storage)\n",
      "/home/theng/miniconda3/envs/textgen/lib/python3.10/site-packages/safetensors/torch.py:99: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  with safe_open(filename, framework=\"pt\", device=device) as f:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting the Embeddings of a Token\n",
    "\n",
    "The first step is to get the embeddings of a token. You can do this by first isolating the embeddings layer from the loaded GPT-2 Model. We then use the weights from the embedd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Isolate the embeddings layer from the GPT 2 model\n",
    "embeddings_layer = model.wte\n",
    "\n",
    "def get_embedding(word: str, embeddings_layer: torch.nn.Embedding) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Given a word, return its vector embedding from the embedding layer in GPT-2.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word : str\n",
    "        The word to be embedded.\n",
    "    embeddings_layer : torch.nn.Embedding\n",
    "        The embeddings layer of the pre-trained GPT-2 model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        The tensor representation of the word's vector embedding.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(word)\n",
    "    with torch.no_grad():\n",
    "        word_embedding = embeddings_layer.weight[tokens, :]\n",
    "    return word_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nearest Tokens\n",
    "\n",
    "The next step is to be able to determine the nearest token to a given arbitrary embedding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Euclidean Distance for Similarity\n",
    "\n",
    "An intuitive metric for checking for similarity of token embeddings is Euclidean distance. In the `closest_token` function, we are given a single token's embedding and we determine the Euclidean distance between the given vector with all embeddings in the embeddings layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def closest_token(embedding: torch.Tensor, embeddings_layer: torch.nn.Embedding) -> int:\n",
    "    \"\"\"\n",
    "    Given an embedding, return the token id with the most similar embedding from the GPT-2 model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding : torch.Tensor\n",
    "        The tensor representation of a word's vector embedding.\n",
    "    embeddings_layer : torch.nn.Embedding\n",
    "        The embeddings layer of the pre-trained GPT-2 model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The token id with the most similar embedding.\n",
    "    \"\"\"\n",
    "    embeddings = embeddings_layer.weight\n",
    "    # Calculate the Euclidean distance\n",
    "    distances = torch.norm(embeddings - embedding, dim=1)  \n",
    "    # Get the token id of the smallest distance\n",
    "    closest_token_id = distances.argmin().item()  \n",
    "    return closest_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Cosine Similarity\n",
    "\n",
    "A common alternative metric for similarity is cosine similarity, which is more dependent on the direction of the embedding rather than the position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def closest_token_cosine(embedding: torch.Tensor, embeddings_layer: torch.nn.Embedding) -> int:\n",
    "    \"\"\"\n",
    "    Given an embedding, return the token id with the most similar embedding from the GPT-2 model,\n",
    "    measured by cosine similarity.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding : torch.Tensor\n",
    "        The tensor representation of a word's vector embedding.\n",
    "    embeddings_layer : torch.nn.Embedding\n",
    "        The embeddings layer of the pre-trained GPT-2 model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The token id with the most similar embedding.\n",
    "    \"\"\"\n",
    "    embeddings = embeddings_layer.weight\n",
    "    # We use cosine_similarity from sklearn, which needs 2D arrays. Reshape our vectors accordingly.\n",
    "    embedding = embedding.reshape(1, -1)\n",
    "    embeddings = embeddings.detach().numpy()\n",
    "    similarities = cosine_similarity(embedding, embeddings)\n",
    "    # Get the token id of the largest similarity\n",
    "    closest_token_id = np.argmax(similarities)  \n",
    "    return closest_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(w1: str, w2: str, w3: str, embeddings_layer: torch.nn.Embedding, cosine_sim=False) -> str:\n",
    "    \"\"\"\n",
    "    Given three words, find a word that is related to the third word in the same way the second word is \n",
    "    related to the first by manipulating word embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w1 : str\n",
    "        The first word.\n",
    "    w2 : str\n",
    "        The second word.\n",
    "    w3 : str\n",
    "        The third word.\n",
    "    embeddings_layer : torch.nn.Embedding\n",
    "        The embeddings layer of the pre-trained GPT-2 model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The token that completes the analogy.\n",
    "    \"\"\"\n",
    "    # Ensure that the embeddings have the correct shape\n",
    "    embed_1 = get_embedding(w1, embeddings_layer)\n",
    "    embed_2 = get_embedding(w2, embeddings_layer)\n",
    "    embed_3 = get_embedding(w3, embeddings_layer)\n",
    "    \n",
    "    # Ensure that the words result in single token embeddings\n",
    "    assert(embed_1.shape[0] == 1)\n",
    "    assert(embed_2.shape[0] == 1)\n",
    "    assert(embed_3.shape[0] == 1)\n",
    "\n",
    "    if cosine_sim:\n",
    "        closest_token_id = closest_token_cosine(embed_2 - embed_1 + embed_3, model.wte)\n",
    "    else:\n",
    "        closest_token_id = closest_token(embed_2 - embed_1 + embed_3, model.wte)\n",
    "\n",
    "    return tokenizer.decode([closest_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean: money\n",
      "Cosine: money\n"
     ]
    }
   ],
   "source": [
    "print(\"Euclidean:\",analogy(\"bank\", \"money\", \"bank\", embeddings_layer, False))\n",
    "print(\"Cosine:\"   ,analogy(\"bank\", \"money\", \"bank\", embeddings_layer, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean: library\n",
      "Cosine: library\n"
     ]
    }
   ],
   "source": [
    "print(\"Euclidean:\",analogy(\"bank\", \"money\", \"library\", embeddings_layer, False))\n",
    "print(\"Cosine:\"   ,analogy(\"bank\", \"money\", \"library\", embeddings_layer, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean: school\n",
      "Cosine: school\n"
     ]
    }
   ],
   "source": [
    "print(\"Euclidean:\",analogy(\"bank\", \"money\", \"school\", embeddings_layer, False))\n",
    "print(\"Cosine:\"   ,analogy(\"bank\", \"money\", \"school\", embeddings_layer, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice in most cases, the linear relationships that the `analogy` function assumes exists between the embeddings learned by GPT-2 does not necessarily exist. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83e1b4289aecf61447277aaee19b289366aa10c64a5f8653a2fa2f79edd63963"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 ('textgen')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
